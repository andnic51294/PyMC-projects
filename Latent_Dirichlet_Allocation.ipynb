{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Latent Dirichlet Allocation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!jupyter nbconvert --to html /content/LatentDirichletAllocation.ipynb.ipynb"
      ],
      "metadata": {
        "id": "fxrfwfpV38G4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94DfZgdfTEqj"
      },
      "outputs": [],
      "source": [
        "!pip install pymc\n",
        "!pip install git+https://github.com/codelucas/newspaper.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TASK 1"
      ],
      "metadata": {
        "id": "GXyTxI8Is4fB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Function definitions**"
      ],
      "metadata": {
        "id": "NLcDPW2Pzu9r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pymc as pm\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "import seaborn as sns\n",
        "sns.set_context(\"poster\")\n",
        "\n",
        "def flatten(t):\n",
        "  return [item for sublist in t for item in sublist]\n",
        "\n",
        "def lemmatize_stemming(text):                                                   \n",
        "  ps = nltk.stem.porter.PorterStemmer()\n",
        "  return ps.stem(text)\n",
        "\n",
        "# Tokenize and lemmatize the text\n",
        "def preprocess(text):\n",
        "  result=[]\n",
        "  for token in gensim.utils.simple_preprocess(text) :\n",
        "      if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
        "          result.append(lemmatize_stemming(token))\n",
        "          \n",
        "  return result\n",
        "\n",
        "def to_one_word_array(v):\n",
        "    docs = list()\n",
        "    for w in v.toarray():\n",
        "        words = list()\n",
        "        for d, n in enumerate(w):\n",
        "            for j in range(n):\n",
        "                words.append(d)\n",
        "        docs.append(words)\n",
        "    return docs\n",
        "  \n",
        "\n",
        "def plot_words_per_topic(Z,ax=None):\n",
        "  if ax is None:\n",
        "      plt.clf()\n",
        "      fig, ax = plt.subplots(1,1)\n",
        "  words = Z\n",
        "  topic_dist = dict()\n",
        "  for k_i in words:\n",
        "      for k in k_i:\n",
        "          if k not in topic_dist:\n",
        "              topic_dist[k] = 0\n",
        "          topic_dist[k] += 1\n",
        "  ax.bar(topic_dist.keys(), topic_dist.values())\n",
        "  ax.set_xlabel(\"Topics\")\n",
        "  ax.set_ylabel(\"Counts\")\n",
        "  ax.set_title(\"Document words per topics\")\n",
        "  plt.show()\n",
        "\n",
        "def plot_word_dist(phi, ax=None):\n",
        "  topics = phi.value\n",
        "  if ax is None:\n",
        "      plt.clf()\n",
        "      fig, ax = plt.subplots((len(topics)+1)/2, 2, figsize=(10,10))\n",
        "  for i, t in enumerate(topics):\n",
        "      ax[i/2][i%2].bar(range(len(t[0])), t[0])\n",
        "      ax[i/2][i%2].set_title(\"Topic %s\" % i)\n",
        "  plt.suptitle(\"Vocab word proportions per topic\")\n",
        "  fig.subplots_adjust(hspace=0.5, wspace=0.5)\n",
        "\n",
        "def plot_data(self):\n",
        "  plt.clf()\n",
        "  plt.matshow(data, fignum=1000, cmap=plt.cm.Reds)\n",
        "  plt.gca().set_aspect('auto')\n",
        "  plt.xlabel(\"Words\")\n",
        "  plt.ylabel(\"Documents\")"
      ],
      "metadata": {
        "id": "vOcUqNtrxmiF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c86649b0-7a39-4486-ed64-deb6f5d062e4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Topic links**"
      ],
      "metadata": {
        "id": "3uAxiGphzde-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from newspaper import Article\n",
        "\n",
        "sport =      ['https://en.wikipedia.org/wiki/Cricket',\n",
        "              'https://en.wikipedia.org/wiki/Tennis',\n",
        "              'https://en.wikipedia.org/wiki/Football',\n",
        "              'https://en.wikipedia.org/wiki/Basketball']\n",
        "\n",
        "electronic = ['https://en.wikipedia.org/wiki/Capacitor',\n",
        "              'https://en.wikipedia.org/wiki/Integrated_circuit',\n",
        "              'https://en.wikipedia.org/wiki/Resistor',\n",
        "              'https://en.wikipedia.org/wiki/Transistor']\n",
        "\n",
        "planet =     ['https://en.wikipedia.org/wiki/Venus',\n",
        "              'https://en.wikipedia.org/wiki/Mars',\n",
        "              'https://en.wikipedia.org/wiki/Uranus',\n",
        "              'https://en.wikipedia.org/wiki/Jupiter']\n",
        "\n",
        "religion =   ['https://en.wikipedia.org/wiki/Christianity',\n",
        "              'https://en.wikipedia.org/wiki/Islam',\n",
        "              'https://en.wikipedia.org/wiki/Judaism',\n",
        "              'https://en.wikipedia.org/wiki/Buddhism']\n",
        "\n",
        "\n",
        "topic_links = {\"sport\":sport, \"religion\": religion,  \"planet\":planet, \"electronic\":electronic}"
      ],
      "metadata": {
        "id": "IGxHu3LBz8U8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Text preprocessing: stemming and special characters removal**"
      ],
      "metadata": {
        "id": "IcGYZQRV0XjD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topic_texts = []\n",
        "preprocessed_files = []\n",
        "\n",
        "for topic in topic_links:\n",
        "  topic_texts.clear()\n",
        "  for article_link in topic_links[topic]:\n",
        "    article = Article(url=\"%s\" % article_link)\n",
        "    article.download()\n",
        "    article.parse()\n",
        "    pp_text = preprocess(article.text)  #stem every word and remove special characters\n",
        "    length = 500                                                                \n",
        "    txt = \" \".join(pp_text[0:length])   #use the first 100 eligibile words from every wikipedia page\n",
        "    topic_texts.append(txt.split())\n",
        "  temp = topic_texts.copy()\n",
        "  preprocessed_files.append(temp)       #save all preprocessed texts in preprocesed_files\n"
      ],
      "metadata": {
        "id": "oi4rLq-f3dUF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Text processing: remove sparse words**"
      ],
      "metadata": {
        "id": "EdtD5dW3ImRB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "processed_files = []                                          \n",
        "filtered_topic = []\n",
        "final_topic = []\n",
        "K = 4    #number of topics\n",
        "\n",
        "print(\"Dictionaries for all %i topics. Topic index is just related to the order of processing and doesn't reflect the final clustering of documents on topics.\"%K)\n",
        "for idx,topic_files in enumerate(preprocessed_files):\n",
        "  n = len(topic_files)       #number of documents in the topic\n",
        "  unique_files = flatten([list(set(file)) for file in topic_files])    #append unique words from every document\n",
        "  dictionary = dict(Counter(unique_files))     #create dictionary of the unique words\n",
        "\n",
        "  #keep words that are common in at least a half of the documents from the same topic\n",
        "  words_to_keep = [pair[0] for pair in list(filter(lambda x: x[1] > n/2,sorted(dictionary.items(), key=lambda x: x[1], reverse=True)))] \n",
        "  for i,doc in enumerate(topic_files):\n",
        "    temp = [i for i in doc if i in words_to_keep]\n",
        "    filtered_topic.append(temp)\n",
        "\n",
        "  # dictionary of filtered words for this particular topic\n",
        "  filtered_dictionary = dict(Counter(flatten(filtered_topic)))\n",
        "\n",
        "  # set threshold for word incidence to 1% of the total number of words for this topic\n",
        "  sparsity_threshold = len(flatten(filtered_topic))*0.01     \n",
        "\n",
        "  # save the words that have incidence higher than the sparcity threshold    \n",
        "  dense_words = [pair[0] for pair in list(filter(lambda x: x[1] > sparsity_threshold,sorted(filtered_dictionary.items(), key=lambda x: x[1], reverse=True)))]  \n",
        "  for i,doc in enumerate(filtered_topic):\n",
        "    temp = [i for i in doc if i in dense_words]\n",
        "    final_topic.append(temp)\n",
        "\n",
        "  temp = final_topic.copy()                                             \n",
        "  processed_files.append(temp)     #save all processed texts in processed_files\n",
        "  final_dictionary = dict(Counter(flatten(final_topic)))\n",
        "  print(\"dictionary for topic %i: \"%idx, {k: v for k, v in sorted(final_dictionary.items(), key=lambda item: item[1], reverse=True)})\n",
        "  filtered_topic.clear()\n",
        "  final_topic.clear()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKQVpikVI2gE",
        "outputId": "0ad225cf-4873-444d-fecb-f50647d5d86b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dictionaries for all 4 topics. Topic index is just related to the order of processing and doesn't reflect the final clustering of documents on topics.\n",
            "dictionary for topic 0:  {'ball': 70, 'game': 54, 'play': 45, 'footbal': 41, 'player': 34, 'team': 30, 'sport': 23, 'centuri': 17, 'court': 15, 'point': 14, 'origin': 13, 'club': 12, 'rule': 12, 'world': 12, 'field': 11, 'england': 11, 'popular': 11, 'goal': 10, 'modern': 10, 'score': 9, 'includ': 9, 'line': 8, 'refer': 8, 'call': 7, 'time': 7, 'mean': 7, 'histori': 6, 'earli': 6}\n",
            "dictionary for topic 1:  {'christian': 33, 'religion': 26, 'teach': 22, 'term': 16, 'centuri': 15, 'earli': 15, 'world': 14, 'tradit': 14, 'practic': 14, 'mean': 13, 'follow': 12, 'includ': 12, 'life': 11, 'religi': 11, 'abraham': 9, 'popul': 9, 'refer': 9, 'form': 9, 'known': 7, 'major': 7, 'call': 7, 'later': 7, 'state': 7, 'east': 7, 'asia': 7, 'faith': 7, 'origin': 7, 'monotheist': 6, 'gener': 6, 'exist': 6, 'africa': 6, 'belief': 6, 'interpret': 6, 'histor': 6}\n",
            "dictionary for topic 2:  {'planet': 71, 'earth': 52, 'surfac': 36, 'venu': 28, 'solar': 22, 'uranu': 22, 'atmospher': 22, 'orbit': 18, 'observ': 17, 'time': 15, 'temperatur': 14, 'like': 14, 'mass': 13, 'giant': 11, 'form': 10, 'moon': 9, 'water': 9, 'object': 8, 'appear': 8, 'mytholog': 7, 'mercuri': 7, 'terrestri': 7, 'similar': 7, 'planetari': 7, 'includ': 7, 'nearli': 7, 'suggest': 7, 'model': 7}\n",
            "dictionary for topic 3:  {'circuit': 45, 'power': 35, 'electr': 32, 'integr': 20, 'electron': 19, 'devic': 17, 'compon': 16, 'work': 16, 'current': 15, 'field': 13, 'invent': 13, 'materi': 11, 'voltag': 11, 'flow': 11, 'effect': 10, 'high': 10, 'termin': 9, 'common': 9, 'amplifi': 9, 'design': 8, 'construct': 8, 'metal': 8, 'connect': 8, 'time': 8, 'radio': 8, 'exampl': 7, 'edit': 7, 'requir': 7, 'packag': 7, 'discret': 7, 'passiv': 6, 'physic': 6, 'type': 6, 'form': 6, 'appli': 6, 'modern': 6, 'similar': 6, 'unit': 6}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Vectorize words**"
      ],
      "metadata": {
        "id": "Rpk7fvJcPQFx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "processed_docs = [\" \".join(file) for file in flatten(processed_files)]          \n",
        "v = vectorizer.fit_transform(processed_docs)\n",
        "inv_voc = {v: k for k, v in vectorizer.vocabulary_.items()}   #create vocabulary of all the processed words\n",
        "\n",
        "documents = to_one_word_array(v)                                   "
      ],
      "metadata": {
        "id": "B8Dh5qzkLA1W"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Total number of words in all the documents:          \",len(flatten(documents)))\n",
        "print(\"Total number of distinct words in all the documents: \",len(inv_voc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IjqgAJeLLGJO",
        "outputId": "65a96a62-f82b-4b78-ee47-28dc77d843ed"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of words in all the documents:           1795\n",
            "Total number of distinct words in all the documents:  112\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Setting the model parameters**"
      ],
      "metadata": {
        "id": "vesTvNAaQ4hb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## α and β represent the prior beliefs about the corpus befor the model is trained\n",
        "\n",
        "\n",
        "*  A low value for α was chosen because every document is composed mainly of one topic. The majority of the probablity mass is in the corners of the simplex.\n",
        "\n"
      ],
      "metadata": {
        "id": "viH4I3O0ln7h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "                                                                                \n",
        "K = 4                                  # number of topics\n",
        "V = len(inv_voc)                       # number of words in the vocabulary\n",
        "M = len(documents)                     # number of documents  \n",
        "N_m = [len(doc) for doc in documents]  # number of words in every doc            \n",
        " \n",
        "                                                  \n",
        "iterations = 5000\n",
        "burn_in = 0\n",
        "\n",
        "# a small value of alpha was chosen because documents are made of one or a few topics.\n",
        "alpha = np.ones(K) * 1/K \n",
        "beta = np.ones(V) * 1/K                                                         \n",
        "print(\"K = \", K)\n",
        "print(\"V = \", V)\n",
        "print(\"M = \", M)"
      ],
      "metadata": {
        "id": "qqrk_0RzmYSA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d06a582-52b9-4477-d332-08bd3bcca6b8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "K =  4\n",
            "V =  112\n",
            "M =  16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **LDA Model**"
      ],
      "metadata": {
        "id": "DTVK2Uny3CB7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# document distribution over topics\n",
        "# completed Dirichlet was used in order to add an additional probability which will make the sum of all probabilities to be alpha\n",
        "theta = pm.Container([pm.CompletedDirichlet(\"theta_%i\" % d, pm.Dirichlet(\"ttheta_%i\" % d, theta=alpha)) for d in range(M)])    \n",
        "                                                                                                                                \n",
        "# topic distribution over words\n",
        "# completed Dirichlet was used in order to add an additional probability which will make the sum of al probabilities to be beta\n",
        "phi = pm.Container([pm.CompletedDirichlet(\"phi_%s\" % k, pm.Dirichlet(\"pphi_%s\" % k, theta=beta)) for k in range(K)])  \n",
        "                                                                                                                      \n",
        "Wd = [len(doc) for doc in documents]    #list with length of all the documents                                                                                            \n",
        "\n",
        "# Will draw words from every document and it will assign a topic according to the probabilities from theta\n",
        "Z = pm.Container([pm.Categorical(\"z_%i\" % d,\n",
        "                                      p = theta[d],                                                              \n",
        "                                      size = Wd[d],                                                              \n",
        "                                      value = np.random.randint(K,size=Wd[d]))      \n",
        "                        for d in range(M)])\n",
        "\n",
        "\n",
        "#given the topic draws in z,the model will select a random word out of its topic distribution in phi\n",
        "W = pm.Container([pm.Categorical(\"w_%i_%i\" % (d,i),                                          \n",
        "                                      p = pm.Lambda(\"phi_z_%i_%i\" % (d,i),                                        \n",
        "                                                    lambda z=Z[d][i], phi=phi : phi[z]),\n",
        "                                      value=documents[d][i],\n",
        "                                      observed=True)  \n",
        "                      for d in range(M) for i in range(Wd[d])])\n",
        "\n",
        "model = pm.Model([theta, phi, Z, W])\n",
        "mcmc = pm.MCMC(model)\n",
        "mcmc.sample(iterations, burn=burn_in)"
      ],
      "metadata": {
        "id": "8mHozRNVbjpS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6db393a5-f5ae-4574-fab6-d3e72b2e08f0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pymc/MCMC.py:81: UserWarning: Instantiating a Model object directly is deprecated. We recommend passing variables directly to the Model subclass.\n",
            "  warnings.warn(message)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " [-----------------100%-----------------] 5000 of 5000 complete in 132.7 sec"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.clf()\n",
        "fig, ax = plt.subplots(1,1)\n",
        "words = Z.value\n",
        "topic_dist = dict()\n",
        "for k_i in words:\n",
        "    for k in k_i:\n",
        "        if k not in topic_dist:\n",
        "            topic_dist[k] = 0\n",
        "        topic_dist[k] += 1\n",
        "ax.bar(topic_dist.keys(), topic_dist.values())\n",
        "ax.set_xlabel(\"Topics\")\n",
        "ax.set_ylabel(\"Counts\")\n",
        "ax.set_title(\"Document words per topics\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "id": "U7gDO3muA6jo",
        "outputId": "590bd8ea-9ec6-4ebd-d06a-efbad2b7198f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Top 5 words for every topic:\\n\")\n",
        "topics = []\n",
        "t=[]\n",
        "for k in range(K):\n",
        "  t.clear()\n",
        "  for i, j in enumerate(mcmc.trace('phi_%i'%k)[-1][0]):\n",
        "    t.append([inv_voc[i], j])\n",
        "  temp = t.copy()\n",
        "  temp.sort(key=lambda x:x[1], reverse = True)\n",
        "  print(\"Topic %i: \"%k)\n",
        "  for pair in temp[:5]:\n",
        "    print(pair)\n",
        "  print(\"\")"
      ],
      "metadata": {
        "id": "iSUH85krixqM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf8fc5e0-fe6f-4f28-8c69-03162cecb937"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 words for every topic:\n",
            "\n",
            "Topic 0: \n",
            "['metal', 0.12671189865696508]\n",
            "['mean', 0.1090590556289564]\n",
            "['type', 0.07378044890469142]\n",
            "['score', 0.055320975795624984]\n",
            "['compon', 0.05222727442383175]\n",
            "\n",
            "Topic 1: \n",
            "['life', 0.0824754488192014]\n",
            "['religion', 0.06754011287963439]\n",
            "['physic', 0.055385916035098355]\n",
            "['suggest', 0.05013077797576808]\n",
            "['planet', 0.04679128721612772]\n",
            "\n",
            "Topic 2: \n",
            "['mytholog', 0.12549203289657004]\n",
            "['earli', 0.07383875687939977]\n",
            "['model', 0.0637098021584985]\n",
            "['form', 0.061002301873164716]\n",
            "['game', 0.05568013436150991]\n",
            "\n",
            "Topic 3: \n",
            "['form', 0.18246705029527546]\n",
            "['christian', 0.07367738875206514]\n",
            "['exist', 0.06300533309840058]\n",
            "['atmospher', 0.05202727074399172]\n",
            "['line', 0.048638095129878996]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "counts = []\n",
        "print(\"Topic prediction for each document:\\n\")\n",
        "for d in range(len(documents)):\n",
        "    # save the last 100 traces for z\n",
        "    pred = mcmc.trace('z_%i'%d)[-100:]                                          \n",
        "    for p in pred:\n",
        "      # create list of list with the results of every word topic drew from every sentence.\n",
        "      counts.append(np.bincount(p, minlength=4))                               \n",
        "    avg_count = list(np.sum(counts,axis=0)/len(pred))\n",
        "    print(\"Document %i -> \"%d,\" Topic %i\" %avg_count.index(max(avg_count)))\n",
        "    print(\"Topic distribution for document %i\"%d,avg_count)\n",
        "    print(\"\")\n",
        "    counts.clear()"
      ],
      "metadata": {
        "id": "g1z3INjokRT6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "126f1f77-81b4-4b74-895a-1d52ce6c72f1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic prediction for each document:\n",
            "\n",
            "Document 0 ->   Topic 0\n",
            "Topic distribution for document 0 [38.48, 29.28, 30.59, 5.65]\n",
            "\n",
            "Document 1 ->   Topic 1\n",
            "Topic distribution for document 1 [1.38, 74.62, 3.0, 52.0]\n",
            "\n",
            "Document 2 ->   Topic 3\n",
            "Topic distribution for document 2 [62.58, 28.42, 14.0, 64.0]\n",
            "\n",
            "Document 3 ->   Topic 2\n",
            "Topic distribution for document 3 [39.0, 0.0, 55.0, 14.0]\n",
            "\n",
            "Document 4 ->   Topic 1\n",
            "Topic distribution for document 4 [17.26, 54.65, 0.09, 31.0]\n",
            "\n",
            "Document 5 ->   Topic 1\n",
            "Topic distribution for document 5 [0.0, 33.05, 32.95, 33.0]\n",
            "\n",
            "Document 6 ->   Topic 3\n",
            "Topic distribution for document 6 [17.0, 7.26, 25.23, 41.51]\n",
            "\n",
            "Document 7 ->   Topic 2\n",
            "Topic distribution for document 7 [18.86, 2.22, 35.32, 19.6]\n",
            "\n",
            "Document 8 ->   Topic 1\n",
            "Topic distribution for document 8 [52.04, 81.38, 2.58, 31.0]\n",
            "\n",
            "Document 9 ->   Topic 2\n",
            "Topic distribution for document 9 [16.0, 16.96, 44.0, 9.04]\n",
            "\n",
            "Document 10 ->   Topic 2\n",
            "Topic distribution for document 10 [19.43, 0.57, 56.77, 31.23]\n",
            "\n",
            "Document 11 ->   Topic 0\n",
            "Topic distribution for document 11 [55.52, 13.48, 0.0, 32.0]\n",
            "\n",
            "Document 12 ->   Topic 1\n",
            "Topic distribution for document 12 [34.57, 45.43, 2.0, 26.0]\n",
            "\n",
            "Document 13 ->   Topic 2\n",
            "Topic distribution for document 13 [18.0, 44.0, 52.0, 9.0]\n",
            "\n",
            "Document 14 ->   Topic 3\n",
            "Topic distribution for document 14 [27.89, 16.77, 8.34, 65.0]\n",
            "\n",
            "Document 15 ->   Topic 3\n",
            "Topic distribution for document 15 [24.0, 5.0, 26.12, 47.88]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TASK 2"
      ],
      "metadata": {
        "id": "qBQDEIkMsyh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Topic-based similarity between documents**\n"
      ],
      "metadata": {
        "id": "55SaGhfVpwKd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cosine Similarity**  captures the orientation of the documents not the magnitude and 2 documents which have a big difference in number of words could score high on the similarity if the sets of words for both have many elements in common."
      ],
      "metadata": {
        "id": "te5DS0mQMh64"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "doc_names = ['Sports_1','Sports_2','Sports_3','Sports_4', 'Electronics_1', 'Electronics_2', 'Electronics_3', 'Electronics_4', 'Planets_1', 'Planets_2', 'Planets_3', 'Planets_4', 'Religions_1', 'Religions_2', 'Religions_3', 'Religions_4']\n",
        "\n",
        "doc_term_matrix = v.todense()\n",
        "words_df = pd.DataFrame(doc_term_matrix, \n",
        "                  columns=vectorizer.get_feature_names(), \n",
        "                  index = doc_names)\n",
        "\n",
        "print(\"Cosine similarity matrix\\n\")\n",
        "similarity = cosine_similarity(words_df, words_df)\n",
        "sim_df = pd.DataFrame(similarity, columns = doc_names)\n",
        "sim_df.insert(0,\"---------\",doc_names, allow_duplicates=True)\n",
        "sim_df = sim_df.set_index(\"---------\")\n",
        "print(sim_df.to_string())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IfPaX_-7Lyy0",
        "outputId": "2c25867c-614b-4f10-99ed-091b491a1561"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine similarity matrix\n",
            "\n",
            "               Sports_1  Sports_2  Sports_3  Sports_4  Electronics_1  Electronics_2  Electronics_3  Electronics_4  Planets_1  Planets_2  Planets_3  Planets_4  Religions_1  Religions_2  Religions_3  Religions_4\n",
            "---------                                                                                                                                                                                                        \n",
            "Sports_1       1.000000  0.807566  0.603160  0.834864       0.062369       0.133510       0.069877       0.097548   0.009841   0.006782   0.008452   0.025335     0.031211     0.013937     0.002084     0.068537\n",
            "Sports_2       0.807566  1.000000  0.540553  0.671859       0.070801       0.115379       0.058151       0.120021   0.012347   0.006078   0.010100   0.029193     0.027971     0.032474     0.002802     0.035830\n",
            "Sports_3       0.603160  0.540553  1.000000  0.620915       0.049771       0.082598       0.039830       0.068544   0.001149   0.003960   0.001645   0.006340     0.009112     0.005968     0.000000     0.024178\n",
            "Sports_4       0.834864  0.671859  0.620915  1.000000       0.029390       0.065232       0.036417       0.028650   0.007180   0.000000   0.005139   0.013204     0.011645     0.005932     0.001901     0.014325\n",
            "Electronics_1  0.062369  0.070801  0.049771  0.029390       1.000000       0.455552       0.410757       0.309665   0.001862   0.006416   0.002665   0.010271     0.000000     0.000000     0.000000     0.000000\n",
            "Electronics_2  0.133510  0.115379  0.082598  0.065232       0.455552       1.000000       0.765222       0.545227   0.001898   0.013080   0.008150   0.066308     0.024623     0.005375     0.012057     0.000000\n",
            "Electronics_3  0.069877  0.058151  0.039830  0.036417       0.410757       0.765222       1.000000       0.490623   0.001987   0.013691   0.008531   0.069409     0.025774     0.005627     0.012621     0.000000\n",
            "Electronics_4  0.097548  0.120021  0.068544  0.028650       0.309665       0.545227       0.490623       1.000000   0.004936   0.018708   0.008478   0.041749     0.006403     0.001398     0.003136     0.000000\n",
            "Planets_1      0.009841  0.012347  0.001149  0.007180       0.001862       0.001898       0.001987       0.004936   1.000000   0.784208   0.338090   0.520042     0.018634     0.012203     0.003733     0.008525\n",
            "Planets_2      0.006782  0.006078  0.003960  0.000000       0.006416       0.013080       0.013691       0.018708   0.784208   1.000000   0.432783   0.637882     0.013134     0.006690     0.002144     0.008813\n",
            "Planets_3      0.008452  0.010100  0.001645  0.005139       0.002665       0.008150       0.008531       0.008478   0.338090   0.432783   1.000000   0.678412     0.019398     0.011116     0.004453     0.008542\n",
            "Planets_4      0.025335  0.029193  0.006340  0.013204       0.010271       0.066308       0.069409       0.041749   0.520042   0.637882   0.678412   1.000000     0.065415     0.026521     0.025169     0.009406\n",
            "Religions_1    0.031211  0.027971  0.009112  0.011645       0.000000       0.024623       0.025774       0.006403   0.018634   0.013134   0.019398   0.065415     1.000000     0.447402     0.817932     0.586187\n",
            "Religions_2    0.013937  0.032474  0.005968  0.005932       0.000000       0.005375       0.005627       0.001398   0.012203   0.006690   0.011116   0.026521     0.447402     1.000000     0.429066     0.344062\n",
            "Religions_3    0.002084  0.002802  0.000000  0.001901       0.000000       0.012057       0.012621       0.003136   0.003733   0.002144   0.004453   0.025169     0.817932     0.429066     1.000000     0.412967\n",
            "Religions_4    0.068537  0.035830  0.024178  0.014325       0.000000       0.000000       0.000000       0.000000   0.008525   0.008813   0.008542   0.009406     0.586187     0.344062     0.412967     1.000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Topic prediction for a new document**\n"
      ],
      "metadata": {
        "id": "N_TnMOWDiFcM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Remove stopwords and apply stemming to the document**"
      ],
      "metadata": {
        "id": "U0fKfiMBsNtg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_link = 'https://en.wikipedia.org/wiki/Pluto'\n",
        "\n",
        "new_text = []\n",
        "preprocessed_files = []\n",
        "\n",
        "test_article = Article(url=\"%s\" % new_link)\n",
        "test_article.download()\n",
        "test_article.parse()\n",
        "\n",
        "#stem every word and remove special characters\n",
        "pp_text = preprocess(test_article.text)   \n",
        "length = 500           \n",
        "#use the first 500 eligibile words from every wikipedia page                                                     \n",
        "new_text = \" \".join(pp_text[0:length])   \n",
        "new_doc = pp_text[0:length]"
      ],
      "metadata": {
        "id": "WmTbM8RuiM6p"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Remove sparse words and vectorize the document**"
      ],
      "metadata": {
        "id": "-U2FJ7R3sUxK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_test_doc = []\n",
        "\n",
        "# dictionary of filtered words for this particular topic\n",
        "temp_dictionary = dict(Counter(new_doc))                                        \n",
        "\n",
        "# set threshold for word incidence to 1% of the number of words \n",
        "sparcity_threshold = len(new_doc)*0.01                                          \n",
        "\n",
        "# save the words that have incidence higher than the sparcity threshold\n",
        "dense_words = [pair[0] for pair in list(filter(lambda x: x[1] > sparcity_threshold,sorted(temp_dictionary.items(), key=lambda x: x[1], reverse=True)))]  \n",
        "final_test_doc = [i for i in new_doc if i in dense_words]\n"
      ],
      "metadata": {
        "id": "efr_0YiwoiVI"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_words = [inv_voc[value] for value in inv_voc]\n",
        "\n",
        "#extract the words that are that can be found in the vocabulary used for training\n",
        "common_words = [i for i in final_test_doc if i in test_words]                   \n",
        "test_words = \" \".join(common_words)\n",
        "\n",
        "#create a new preprocessed corpus with training and test documents\n",
        "all_words = processed_docs.copy()\n",
        "all_words.append(test_words)                                                    \n",
        "\n",
        "#vectorize all the words documents\n",
        "#and keep only the test documents\n",
        "v_all = vectorizer.fit_transform(all_words)                                     \n",
        "all_documents = to_one_word_array(v_all)\n",
        "test_documents = [all_documents[-1]]                                            \n",
        "test_text = \" \".join(common_words)"
      ],
      "metadata": {
        "id": "7iIeb7U40q0r"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Test model**"
      ],
      "metadata": {
        "id": "vO42zasi79L_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wd_test = [len(doc) for doc in test_documents]        \n",
        "\n",
        "#both priors will be set to 1 for the training set\n",
        "alpha = np.ones(K)                                                              \n",
        "beta = np.ones(V)\n",
        "\n",
        "D_test = len(test_documents)\n",
        "wd_test = [len(doc) for doc in test_documents]\n",
        "\n",
        "theta_test = pm.Container([pm.CompletedDirichlet(\"theta_t_%s\" % i, pm.Dirichlet(\"ptheta_t_%s\" % i, theta=alpha)) for i in range(D_test)])    \n",
        "z_test = pm.Container([pm.Categorical('z_t_%s' % d, p = theta_test[d], size=wd_test[d], value=np.random.randint(K, size=wd_test[d])) for d in range(D_test)])\n",
        "   \n",
        "w_test = pm.Container([pm.Categorical(\"w_t_%i_%i\" % (d,i), p = pm.Lambda('phi_z_t_%i_%i' % (d,i), lambda z=z_test[d][i], phi=phi: phi[z]),\n",
        "                    value=test_documents[d][i], observed=True) for d in range(D_test) for i in range(wd_test[d])])\n",
        "\n",
        "model_test = pm.Model([theta_test, z_test, w_test])\n",
        "mcmc_test = pm.MCMC(model_test)\n",
        "mcmc_test.sample(iter=5000, burn=100)"
      ],
      "metadata": {
        "id": "0ogNfzp9-wUA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ed13944-64b2-48ef-ab4e-9fe0fe99ec69"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pymc/MCMC.py:81: UserWarning: Instantiating a Model object directly is deprecated. We recommend passing variables directly to the Model subclass.\n",
            "  warnings.warn(message)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " [-----------------100%-----------------] 5000 of 5000 complete in 4.0 sec"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Topic prediction and the distribution over topics for the test documents**"
      ],
      "metadata": {
        "id": "G1_baO1-VLKh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "counts.clear()\n",
        "\n",
        "counts = []\n",
        "print(\"Topic prediction for the test documents:\\n\")\n",
        "for d in range(len(test_documents)):\n",
        "    # save the last 100 traces for z\n",
        "    pred = mcmc_test.trace('z_t_%i'%d)[-100:]                                   \n",
        "    for p in pred:\n",
        "      # create list of list with the results of every word topic drew from every sentence.\n",
        "      counts.append(np.bincount(p, minlength=4))                                \n",
        "    avg_count = list(np.sum(counts,axis=0)/len(pred))\n",
        "    print(\"Test document #%i -> \"%d,\" Topic %i\" %avg_count.index(max(avg_count)))\n",
        "    print(\"Topic distribution for document %i\"%d,avg_count)\n",
        "    print(\"\")\n",
        "    counts.clear()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ep1GVcIfSLhA",
        "outputId": "8ebfd15d-1c76-48b4-e53c-adad355466d8"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic prediction for the test documents:\n",
            "\n",
            "Test document #0 ->   Topic 2\n",
            "Topic distribution for document 0 [9.41, 6.58, 13.33, 6.68]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Correlated Topic Models** (unfinished)\n"
      ],
      "metadata": {
        "id": "XN7QDrNsYwgS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "K = 4                                   # number of topics\n",
        "V = len(inv_voc)                        # number of words in the vocabulary\n",
        "M = len(documents)                      # number of documents  \n",
        "N_m = [len(doc) for doc in documents]   # number of words in every doc "
      ],
      "metadata": {
        "id": "XU3qADYohW4q"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   The key to the correlated topic model is the logistic normal distribution on the simplex. \n",
        "*   **η** is normally distributed and then mapped to the simplex.\n",
        "*   **f(η)** will be the soft max regression function.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8ppLkzdqSDi9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pymc as pm\n",
        "\n",
        "def softmax(x):\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum(axis=0)\n",
        "\n",
        "\n",
        "Wd = [len(doc) for doc in documents]\n",
        "\n",
        "# mu is a K-dimensional vector\n",
        "mu = flatten(np.ones((1,K),dtype = np.float64)) \n",
        "\n",
        "# generate a random K x K covariance matrix\n",
        "cov_mx = np.random.random_sample(size=(K,K))       \n",
        "# create a random positive definite covariance matrix\\                            \n",
        "pos_def = np.transpose(cov_mx) @ cov_mx                                         \n",
        "# get its inverse   \n",
        "inv_pos_def = np.linalg.inv(pos_def)                                                        \n",
        "\n",
        "# multi-variate normal distribution to be mapped on the simplex from which we will draw the topic assignment\n",
        "eta = pm.MvNormal('eta', mu = mu, tau = inv_pos_def)                            \n",
        "f_eta = softmax(eta.value)   "
      ],
      "metadata": {
        "id": "KBYs1jNerqLD"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# topic distribution over words                                          \n",
        "phi = pm.Container([pm.CompletedDirichlet(\"phi_%s\" % k, pm.Dirichlet(\"pphi_%s\" % k, theta=beta)) for k in range(K)])  \n",
        "\n",
        "#list with length of all the documents\n",
        "Wd = [len(doc) for doc in documents]                                                                                                                                        \n",
        "\n",
        "# Will draw words from every document and it will assign a topic according to the probabilities from f_eta\n",
        "Z = pm.Container([pm.Categorical(\"z_%i\" % d,\n",
        "                                      p = f_eta,                                \n",
        "                                      size = Wd[d],                             \n",
        "                                      value = np.random.randint(K,size=Wd[d])) \n",
        "                        for d in range(M)])\n",
        "\n",
        "\n",
        "W = pm.Container([pm.Categorical(\"w_%i_%i\" % (d,i),                                          \n",
        "                                      p = pm.Lambda(\"phi_z_%i_%i\" % (d,i),\n",
        "                                                    lambda z=Z[d][i], phi=phi : phi[z]),\n",
        "                                      value=documents[d][i],\n",
        "                                      observed=True)  \n",
        "                      for d in range(M) for i in range(Wd[d])])"
      ],
      "metadata": {
        "id": "LuutSfmsfv60"
      },
      "execution_count": 22,
      "outputs": []
    }
  ]
}